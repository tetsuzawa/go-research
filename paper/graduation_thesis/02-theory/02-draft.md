# 理論


## 適応フィルタの基礎

### ウィナーフィルタ

#### ウィナーフィルタの構造

ウィナーフィルタは線形MMSE法（最小平均二乗誤差法）を定常な時系列に適用した特殊型であり、後述の適応アルゴリズムの基礎となる。

ウィナーフィルタのブロック図を図@に示す。

![block_jp](figures/block_jp.png)

@TODO ウィナーフィルタのFIR型の図

ウィナーフィルタの入出力は次式で表される。

$$

y_k = \bm{w}^H \bm{x}_k

$$

ここで、$\bm{w}^H$ は次式で与えられるフィルタ係数ベクトルである。

$$

\bm{w}^H = [w_1^*, \cdots, w_{K}^*]^T

$$

一方、フィルタ入力は次式のような時系列 ${x_k}$ を逆順に並べたものとなる。

$$

\bm{x}_k = [x_k, x_{k-1}, \cdots, x_{k-K+1}]^T

$$

これは、式@により、次式のような時間領域の畳み込み演算を表すためである。

$$

y_k = \sum_{i=1}^K w_i^* x_{k-i+1}

$$

これにより、式@はフィルタ係数${w_i^*}$を持つ時間領域のFIRフィルタを表すことになる。入力時系列{x_k}については、平均値 $E[x_k]=0$ の実数あるいは複素数を仮定している。

ウィナーフィルタで推定すべき値は、実数あるいは複素数のスカラー量$d_k$となる。$d_k$は望みの応答と呼ばれ、フィルタ出力$y_k$により、望みの応答を推定する。すなわち、$\hat{d_k} = y_k$となる。フィルタの推定過程では、次式で定義される推定誤差を、次式で述べる規範のもとに最小化する。

$$

\epsilon_k := d_k - y_k

$$

上述のように、適応フィルタの目的は、入力信号$\bm{x}_k$から$d_k$を推定することである。
これを達成するための過程として、フィルタ$\bm{w}$の係数を決定する学習過程と、式@により$\hat{d}_k$を推定するフィルタリング過程とに分けられる。
フィルタの学習過程では、信号$d_k$を教師として与え、$d_k$と$\bm{x}_k$の関係を表す係数$\bm{w}$を学習する。
一方、フィルタリング過程では、信号$d_k$が未知の場合について、観測値$\bm{x}_k$と学習済みのフィルタ係数$\bm{w}$から、信号の推定値$\hat{d_k} (= y_k)$を得る。学習過程で望みの応答$d_k$をどうやって与えるかは、応用に依存する@。



#### ウィナーフィルタの導出

ウィナーフィルタでは、観測値$\bm{x}_k$から信号$d_k$を推定する。フィルタの学習過程では、次式の二乗平均誤差をコスト関数として最小化する。

$$

J &= E[|\epsilon_k|^2]
  &= E[ (d_k - \bm{w}^H \bm{x}_k) (d_k - \bm{w}^H \bm{x}_k)^H ]
  &= E[ d_k d_k^H] - \bm{w}^H E[\bm{x}_k d_k^H] - E[d_k \bm{x}_k^H ] \bm{w}

$$

ここで次式を定義する。

$$

\sigma_d^2 := E[d_k d_k^H] 、 \bm{r}_{xd} := \bm{x}_k d_k^H 、 \bm{r}_{dx} := d_k \bm{x}_k^H 、 \bm{R}_x := E[\bm{x}_k \bm{x}_k^H]

$$

$\bm{r}_{xd}$および$\bm{r}_{dx}$は、入力ベクトル$\bm{x}_k$と望みの応答$d_k$との相互相関ベクトル、$\bm{R}_u$は、$\bm{x}_k$の自己相関行列である。これらを用いて式@は次式のように書き直せる。


$$

J =\sigma_d^2 + - \bm{w}^H \bm{r}_{xd} -\bm{r}_{dx} \bm{w} + \bm{w}^H \bm{R}_x \bm{w}

$$

コスト関数を\bm{w}^*について偏微分すると次式のようになる。

$$

\frac{\partial J}{\partial \bm{w}^*} = - \bm{r}_{xd} + \bm{R}_x \bm{w}

$$

式@一個前 を$\bm{0}_{K \times 1}$とおくと、次式を得る。

$$

\bm{R}_x \bm{w} = \bm{r}_xd

$$

式@一個前 は、正規方程式あるいはウィナー・ホッフ方程式と呼ばれる。式@一個前 を解くことにより最適フィルタは、次式のように求まる@。

$$

\hat{\bm{w}_{WF}} = \bm{R}_u^{-1} \bm{r}_{xd}

$$

### 適応フィルタにおける最小二乗法（LS法）

適応フィルタにおけるLS法は、ウィナーフィルタの近似解を有限のサンプルから求めるものであり、@RLS 節で述べるRLS法の基礎となっている。

適応フィルタにおけるLS法では、次式に示す誤差の二乗和が最小化される。

$$

J = \sum_{k=1}^{L_s} |\epsilon_k|^2 = \sum_{k=1}^{L_s} |d_k - \bm{w}^H \bm{x}_k|^2

$$

ここで、$L_s$はサンプル数である。ウィナーフィルタの導出過程における期待値をサンプル平均に置き換え、同様に正規方程式を求めると次式が導かれる。

$$

\hat{\bm{R}_x} \bm{w} = \hat{\bm{r}_xd}

$$

これより、LS法における最適解は、次式のようになる。

$$

\hat{\bm{w}_{LS}} = \hat{\bm{R}_x}^{-1} \hat{\bm{r}_xd}

$$

### 最急降下法

@ウィナーフィルタ 節で述べたウィナーフィルタでは、二乗平均誤差を最小とするフィルタ$\bm{w}$を、正規方程式の解として求めた。ここでは、この解を反復法を用いて逐次的に求める。

反復法では、フィルタ係数の初期値を適当に定め、コスト関数$J(\bm{w})$の最小点を目指して、フィルタ係数を少しずつ変化させていく。式@ウィナー偏微分の前 で示したコスト関数の場合、$J(\bm{w})$は$w$についての2次関数となり、下に凸の誤差特性曲面となる。最急降下法では、$k-1$回目の反復におけるフィルタ係数を$\bm{w}_{k-1}$とした場合、$\bm{w} = \bm{w}_{k-1}$での誤差特性曲面の勾配を推定し、この勾配と逆の方向にフィルタ係数を変化させる。$\bm{w} = \bm{w}_{k-1}$における勾配ベクトルは式@ウィナー偏微分 から、次式のように求まる。

$$

\nabra J(\bm{w}_{k-1}) := \frac{\partial J(\bm{w})}{\partial \bm{w}^*} |_{\bm{w} = \bm{w}_{k-1}} = - \bm{r}_{xd} + \bm{R}_x \bm{w}_{k-1}

$$

次回の反復で勾配とは逆方向に係数を変化させるため、フィルタ係数ベクトルの変化分は次式のようになる。

$$

\Delta \bm{w} = - \mu \nabla J(\bm{w}_{k-1})

$$

$\mu$は1回の更新量を決定する性の定数であり、ステップサイズパラメータと呼ばれる。これを用いて、フィルタの更新式は、次式のようになる。

$$

\bm{w}_k &= \bm{w}_{k-1} + \Delta \bm{w}
         &= \bm{w}_{k-1} + \mu (\bm{r}_{xd} - \bm{R}_x \bm{w}_{k-1})

$$

また、式@は次式のように書き直すことができる。


$$

\bm{w}_k &= \bm{w}_{k-1} + \mu (E[\bm{x}_k d_k^*] - E[\bm{x}_k \bm{x}_^H] \bm{w}_{k-1})
         &= \bm{w}_{k-1} + \mu E[\bm{x}_k (d_k^* - \bm{x}_k^H \bm{w}_{k-1})]

$$

ここで、次式の事前推定誤差を定義する。

$$

e_k := d_k - \bm{w}_{k-1}^H \bm{x}_k

$$

式@二個前 に式@一個前 を代入して、次式を得る。


$$

\bm{w}_k = \bm{w}_{k-1} + \mu E[\bm{x}_k e_k^*]

$$


## 代表的な適応アルゴリズム

### 最小二乗法平均法（LMS法）

リアルタイムシステムを構築する場合などは、メモリや演算量に制約があることがある。最急降下法の更新式はシンプルだが、期待値演算をサンプル平均で実装することになり、メモリや演算量を必要とする。この点を改良したのが最小二乗平均法（LMS法）である。LMS法では、式@最急降下法最後における期待値演算を用いた勾配の推定を、次式のように、瞬時値の推定値に置き換える。

$$

\bm{w}_k = \bm{w}_{k-1} + \mu \bm{x}_k e_k^*

$$

また、式@一個前を入力のパワーで正規化したものは、学習同定法（NLMS法）と呼ばれる。NLMS法は、次式の拘束月最適化問題空導くことができる。

$$

min ||\bm{w}_k - \bm{w}_{k-1}||^2 subject to \bm{w}_k^H \bm{x}_k = d_k

$$

この最適化問題は、ラグランジュの未定乗数法を用いて解くことができる。ラグランジュの未定乗数法では、次式のコスト関数を最小化する。

$$

J &= ||\bm{w}_k - \bm{w}_{k-1}||^2  + Re(\lambda (d_k - \bm{w}_k^H \bm{x}_k))
  &= (\bm{w}_k - \bm{w}_{k-1})^H (\bm{w}_k - \bm{w}_{k-1}) + \lambda (d_k - \bm{w}_k^H \bm{x}_k) + (d_k - \bm{w}_k^H \bm{x}_k))^H \lambda^H

$$

コスト関数$J$を$\bm{w}_k^*$について偏微分すると

$$

\frac{\partial J}{\partial \bm{w}_k^*} = \bm{w}_k - \bm{w}_{k-1} - \lambda \bm{x}_k

$$

これを$\bm{0}_{K \times 1}$とすることにより、次式を得る。

$$

\bm{w}_k - \bm{w}_{k-1} = \lambda \bm{x}_k

$$

式@一個前 の左から$\bm{x}_k^H$を乗じ、$\lambda$について解くと

$$

\lambda = \frac{1}{\bm{x}_k^H \bm{x}_k} \bm{x}_k^H (\bm{w}_k - \bm{w}_{k-1})

$$

これに拘束条件（式@NLMSのやつ ）および事前推定誤差（式@最急降下法の最後から一個前 ）

$$

\lambda = \frac{1}{ ||\bm{x}_k||^2 } (d_k^* - \bm{x}_k^H \bm{w}_{k-1}) = \frac{1}{||\bm{x}_k||^2} e_k^*

$$

式@一個前 を再び式@３個前に代入することにより、フィルタの変化量$\Delta \bm{w}$は次式のように求まる。

$$

\Delta \bm{w} = \frac{1}{||\bm{x}_k||^2} \bm{x}_k e_k^*

$$

以上より、NLMS方のフィルタベクトル更新式は次式のようになる。

$$

\bm{w}_k = \bm{w}_{k-1} + \lambda \frac{1}{||\bm{x}_k||^2} \bm{x}_k e_k^*

$$

実際の運用では、入力信号のパワー$||\bm{x}_k||^2$が非常に小さいときに更新式が不安定になるのを防ぐため、$ 1 / ||\bm{x}_k||^2 $ の分母に小さい正の定数 $\alpha$ を加えた、次式を用いる。

$$

\bm{w}_k = \bm{w}_{k-1} + \lambda \frac{1}{\alpha + ||\bm{x}_k||^2} \bm{x}_k e_k^*

$$


### アフィン射影法（APA法）

アフィン射影法は、NLMS法には、NLMS法における拘束付き最適化問題の拘束条件を次式のように複数に拡張することにより導くことができる。

$$

min ||\bm{w}_k - \bm{w}_{k-1}||^2 subject to \bm{w}_k^H \bm{X}_k = d_k

$$

ここで、$\bm{X_k}$（$K \times L_s$の行列）および$\bm{d_k}$（$1 \times L_s$の行ベクトル）は次式で定義される。

$$

\bm{d}_k := [d_{k-L_s+1}, \cdots, d_k]

$$

$$

\bm{X}_k := [\bm{x}_{k-L_s+1}, \cdots, \bm{x}_k]

$$

$$

\bm{x}_k := [\bm{x}_{k}, \cdots, \bm{x}_{k-L_s+1}]^T

$$

拘束条件は、次式の$L_s$個の拘束条件を行列・ベクトル形式で表したものである。

NLMS法と同様にラグランジュの未定乗数法を用いた、最適解を導出により、次式のフィルタ更新式が得られる。



### 再帰最小二乗法（RLS法）

再帰最小二乗法（RLS法）は。LS法の解を再帰的に求める手法である。

サンプル平均による自己相関行列および相互相関ベクトルの推定値$\hat{\bm{R}_x}$および$\hat{\bm{r}_{xd}}$を次式に示す。

$$

\hat{\bm{R}_k} := \sum_{i=1}^k \bm{x}_i \bm{x}_i^H = \bm{X}_{1:k} \bm{X}_{1:k}^H

$$

$$

\hat{\bm{r}_k} := \sum_{i=1}^k \bm{x}_i d_i^* = \bm{X}_{1:k} \bm{d}_{1:k}^H

$$

ここで、$\bm{d}_{1:k}$および$\bm{X}_{1:k}$

$$

\bm{d}_{1:k} := [d_1, \cdots, d_k]

$$

$$

\bm{X}_{1:k} := [\bm{x}_1, \cdots, \bm{x}_k]

$$

$$

\bm{x}_{k} := [x_k, \cdots, u_{k-K+1}]^T

$$

時刻kにおける$\hat{\bm{R}_x}$および$\hat{\bm{r}_x}$は、一時刻前$(k-1)$の値を用いて、次式のように再帰的に表すことができる。

$$

\hat{\bm{R}_k} = \hat{\bm{R}_{k-1}} + \bm{x}_k \bm{x}_k^H

$$

$$

\hat{\bm{r}_k} = \hat{\bm{r}_{k-1}} + \bm{x}_k d_k^*

$$

逆行列の補助定理を用いると、自己相関行列の逆行列$P_k := \hat{\bm{R}_k^{-1}}$も、次式のように再帰的に求めることができる。

$$

\bm{P}_k = \bm{P}_{k-1} - \frac{\bm{P}_{k-1} \bm{x}_k \bm{x}_k^H \bm{P}_{k-1}}{1 + \bm{x}_k^H \bm{P}_{k-1} \bm{x}_k} 

$$

式@二個前、式@一個前 を用いて、求めるべきフィルタ系ルウは、次式のようになる。

$$

\bm{w}_k = \bm{P}_k \hat{\bm{r}_k}

$$

ここで、更新式における演算を簡略化するため、次式のゲインベクトルを定義する。

$$

\bm{g}_k := \frac{\bm{P}_{k-1} \bm{x}_k}{1 + \bm{x}_k^H \bm{P}_{k-1} \bm{x}_k}

$$

これを用いて式@三個前を書き直すと

$$

\bm{P}_k = \bm{P}_{k-1} - \bm{g}_k \bm{x}_k^H \bm{P}_{k-1} = (\bm{I} - \bm{g}_k \bm{x}_k^H) \bm{P}_{k-1}

$$

一方、式@三個前から、次式を得る。

$$

\bm{g}_k = (\bm{I} - \bm{g}_k \bm{x}_k^H) \bm{P}_{k-1} \bm{x}_k

$$

式@二個前、式@一個前を用いて、ゲインベクトルは次式のように書き直せる。

$$

\bm{g}_k = \bm{P}_k \bm{x}_k

$$

式@三個前 および式@一個前 を式@wのやつ に代入することにより、フィルタ係数ベクトル$\bm{w}_k$の更新式は、次式のように求まる。

$$

\bm{w}_k &= \bm{P}_k (\hat{\bm{r}_{k-1}} + \bm{x}_k d_k^*)
         &= \bm{w}_{k-1} + \bm{g}_k (d_k^* - \bm{x}_k^H \bm{w}_{k-1})

$$

最後に、式@一個前 に式@三個前 を代入して、次式の更新式を得る。

$$

\bm{w}_k = \bm{w}_{k-1} + \gm{g}_k e_k^*

$$
