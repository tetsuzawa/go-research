# 理論


## 適応フィルタの基礎

### ウィナーフィルタ

#### ウィナーフィルタの構造

ウィナーフィルタは線形MMSE法（最小平均二乗誤差法）を定常な時系列に適用した特殊型であり、後述の適応アルゴリズムの基礎となる。

ウィナーフィルタのブロック図を図@に示す。

![block_jp](figures/block_jp.png)

ウィナーフィルタの入出力は次式で表される。

$$

y_k = \bm{w}^H \bm{x}_k

$$

ここで、$\bm{w}^H$ は次式で与えられるフィルタ係数ベクトルである。

$$

\bm{w}^H = [w_1^*, \cdots, w_{K}^*]^T

$$

一方、フィルタ入力は次式のような時系列 ${x_k}$ を逆順に並べたものとなる。

$$

\bm{x}_k = [x_k, x_{k-1}, \cdots, x_{k-K+1}]^T

$$

これは、式@により、次式のような時間領域の畳み込み演算を表すためである。

$$

y_k = \sum_{i=1}^K w_i^* x_{k-i+1}

$$

これにより、式@はフィルタ係数${w_i^*}$を持つ時間領域のFIRフィルタを表すことになる。入力時系列{x_k}については、平均値 $E[x_k]=0$ の実数あるいは複素数を仮定している。

ウィナーフィルタで推定すべき値は、実数あるいは複素数のスカラー量$d_k$となる。$d_k$は望みの応答と呼ばれ、フィルタ出力$y_k$により、望みの応答を推定する。すなわち、$\hat{d_k} = y_k$となる。フィルタの推定過程では、次式で定義される推定誤差を、次式で述べる規範のもとに最小化する。

$$

\epsilon_k := d_k - y_k

$$

上述のように、適応フィルタの目的は、入力信号$\bm{x}_k$から$d_k$を推定することである。
これを達成するための過程として、フィルタ$\bm{w}$の係数を決定する学習過程と、式@により$\hat{d}_k$を推定するフィルタリング過程とに分けられる。
フィルタの学習過程では、信号$d_k$を教師として与え、$d_k$と$\bm{x}_k$の関係を表す係数$\bm{w}$を学習する。
一方、フィルタリング過程では、信号$d_k$が未知の場合について、観測値$\bm{x}_k$と学習済みのフィルタ係数$\bm{w}$から、信号の推定値$\hat{d_k} (= y_k)$を得る。学習過程で望みの応答$d_k$をどうやって与えるかは、応用に依存する@。



#### ウィナーフィルタの導出

ウィナーフィルタでは、観測値$\bm{x}_k$から信号$d_k$を推定する。フィルタの学習過程では、次式の二乗平均誤差をコスト関数として最小化する。

$$

J &= E[|\epsilon_k|^2]
  &= E[ (d_k - \bm{w}^H \bm{x}_k) (d_k - \bm{w}^H \bm{x}_k)^H ]
  &= E[ d_k d_k^H] - \bm{w}^H E[\bm{x}_k d_k^H] - E[d_k \bm{x}_k^H ] \bm{w}

$$

ここで次式を定義する。

$$

\sigma_d^2 := E[d_k d_k^H] 、 \bm{r}_{xd} := \bm{x}_k d_k^H 、 \bm{r}_{dx} := d_k \bm{x}_k^H 、 \bm{R}_x := E[\bm{x}_k \bm{x}_k^H]

$$

$\bm{r}_{xd}$および$\bm{r}_{dx}$は、入力ベクトル$\bm{x}_k$と望みの応答$d_k$との相互相関ベクトル、$\bm{R}_u$は、$\bm{x}_k$の自己相関行列である。これらを用いて式@は次式のように書き直せる。


$$

J =\sigma_d^2 + - \bm{w}^H \bm{r}_{xd} -\bm{r}_{dx} \bm{w} + \bm{w}^H \bm{R}_x \bm{w}

$$

コスト関数を\bm{w}^*について偏微分すると次式のようになる。

$$

\frac{\partial J}{\partial \bm{w}^*} = - \bm{r}_{xd} + \bm{R}_x \bm{w}

$$

式@一個前 を$\bm{0}_{K \times 1}$とおくと、次式を得る。

$$

\bm{R}_x \bm{w} = \bm{r}_xd

$$

式@一個前 は、正規方程式あるいはウィナー・ホッフ方程式と呼ばれる。式@一個前 を解くことにより最適フィルタは、次式のように求まる。
