# 理論


## 適応フィルタの基礎

### ウィナーフィルタ

#### ウィナーフィルタの構造

ウィナーフィルタは線形MMSE法（最小平均二乗誤差法）を定常な時系列に適用した特殊型であり、後述の適応アルゴリズムの基礎となる。

ウィナーフィルタのブロック図を図@に示す。

![block_jp](figures/block_jp.png)

@TODO ウィナーフィルタのFIR型の図

ウィナーフィルタの入出力は次式で表される。

$$

y_k = \bm{w}^H \bm{x}_k

$$

ここで、$\bm{w}^H$ は次式で与えられるフィルタ係数ベクトルである。

$$

\bm{w}^H = [w_1^*, \cdots, w_{K}^*]^T

$$

一方、フィルタ入力は次式のような時系列 ${x_k}$ を逆順に並べたものとなる。

$$

\bm{x}_k = [x_k, x_{k-1}, \cdots, x_{k-K+1}]^T

$$

これは、式@により、次式のような時間領域の畳み込み演算を表すためである。

$$

y_k = \sum_{i=1}^K w_i^* x_{k-i+1}

$$

これにより、式@はフィルタ係数${w_i^*}$を持つ時間領域のFIRフィルタを表すことになる。入力時系列{x_k}については、平均値 $E[x_k]=0$ の実数あるいは複素数を仮定している。

ウィナーフィルタで推定すべき値は、実数あるいは複素数のスカラー量$d_k$となる。$d_k$は望みの応答と呼ばれ、フィルタ出力$y_k$により、望みの応答を推定する。すなわち、$\hat{d_k} = y_k$となる。フィルタの推定過程では、次式で定義される推定誤差を、次式で述べる規範のもとに最小化する。

$$

\epsilon_k := d_k - y_k

$$

上述のように、適応フィルタの目的は、入力信号$\bm{x}_k$から$d_k$を推定することである。
これを達成するための過程として、フィルタ$\bm{w}$の係数を決定する学習過程と、式@により$\hat{d}_k$を推定するフィルタリング過程とに分けられる。
フィルタの学習過程では、信号$d_k$を教師として与え、$d_k$と$\bm{x}_k$の関係を表す係数$\bm{w}$を学習する。
一方、フィルタリング過程では、信号$d_k$が未知の場合について、観測値$\bm{x}_k$と学習済みのフィルタ係数$\bm{w}$から、信号の推定値$\hat{d_k} (= y_k)$を得る。学習過程で望みの応答$d_k$をどうやって与えるかは、応用に依存する@。



#### ウィナーフィルタの導出

ウィナーフィルタでは、観測値$\bm{x}_k$から信号$d_k$を推定する。フィルタの学習過程では、次式の二乗平均誤差をコスト関数として最小化する。

$$

J &= E[|\epsilon_k|^2]
  &= E[ (d_k - \bm{w}^H \bm{x}_k) (d_k - \bm{w}^H \bm{x}_k)^H ]
  &= E[ d_k d_k^H] - \bm{w}^H E[\bm{x}_k d_k^H] - E[d_k \bm{x}_k^H ] \bm{w}

$$

ここで次式を定義する。

$$

\sigma_d^2 := E[d_k d_k^H] 、 \bm{r}_{xd} := \bm{x}_k d_k^H 、 \bm{r}_{dx} := d_k \bm{x}_k^H 、 \bm{R}_x := E[\bm{x}_k \bm{x}_k^H]

$$

$\bm{r}_{xd}$および$\bm{r}_{dx}$は、入力ベクトル$\bm{x}_k$と望みの応答$d_k$との相互相関ベクトル、$\bm{R}_u$は、$\bm{x}_k$の自己相関行列である。これらを用いて式@は次式のように書き直せる。


$$

J =\sigma_d^2 + - \bm{w}^H \bm{r}_{xd} -\bm{r}_{dx} \bm{w} + \bm{w}^H \bm{R}_x \bm{w}

$$

コスト関数を\bm{w}^*について偏微分すると次式のようになる。

$$

\frac{\partial J}{\partial \bm{w}^*} = - \bm{r}_{xd} + \bm{R}_x \bm{w}

$$

式@一個前 を$\bm{0}_{K \times 1}$とおくと、次式を得る。

$$

\bm{R}_x \bm{w} = \bm{r}_xd

$$

式@一個前 は、正規方程式あるいはウィナー・ホッフ方程式と呼ばれる。式@一個前 を解くことにより最適フィルタは、次式のように求まる@。

$$

\hat{\bm{w}_{WF}} = \bm{R}_u^{-1} \bm{r}_{xd}

$$

### 適応フィルタにおける最小二乗法（LS法）

適応フィルタにおけるLS法は、ウィナーフィルタの近似解を有限のサンプルから求めるものであり、@RLS 節で述べるRLS法の基礎となっている。

適応フィルタにおけるLS法では、次式に示す誤差の二乗和が最小化される。

$$

J = \sum_{k=1}^{L_s} |\epsilon_k|^2 = \sum_{k=1}^{L_s} |d_k - \bm{w}^H \bm{x}_k|^2

$$

ここで、$L_s$はサンプル数である。ウィナーフィルタの導出過程における期待値をサンプル平均に置き換え、同様に正規方程式を求めると次式が導かれる。

$$

\hat{\bm{R}_x} \bm{w} = \hat{\bm{r}_xd}

$$

これより、LS法における最適解は、次式のようになる。

$$

\hat{\bm{w}_{LS}} = \hat{\bm{R}_x}^{-1} \hat{\bm{r}_xd}

$$

### 最急降下法

@ウィナーフィルタ 節で述べたウィナーフィルタでは、二乗平均誤差を最小とするフィルタ$\bm{w}$を、正規方程式の解として求めた。ここでは、この解を反復法を用いて逐次的に求める。

反復法では、フィルタ係数の初期値を適当に定め、コスト関数$J(\bm{w})$の最小点を目指して、フィルタ係数を少しずつ変化させていく。式@ウィナー偏微分の前 で示したコスト関数の場合、$J(\bm{w})$は$w$についての2次関数となり、下に凸の誤差特性曲面となる。最急降下法では、$k-1$回目の反復におけるフィルタ係数を$\bm{w}_{k-1}$とした場合、$\bm{w} = \bm{w}_{k-1}$での誤差特性曲面の勾配を推定し、この勾配と逆の方向にフィルタ係数を変化させる。$\bm{w} = \bm{w}_{k-1}$における勾配ベクトルは式@ウィナー偏微分 から、次式のように求まる。

$$

\nabra J(\bm{w}_{k-1}) := \frac{\partial J(\bm{w})}{\partial \bm{w}^*} |_{\bm{w} = \bm{w}_{k-1}} = - \bm{r}_{xd} + \bm{R}_x \bm{w}_{k-1}

$$

次回の反復で勾配とは逆方向に係数を変化させるため、フィルタ係数ベクトルの変化分は次式のようになる。

$$

\Delta \bm{w} = - \mu \nabla J(\bm{w}_{k-1})

$$

$\mu$は1回の更新量を決定する性の定数であり、ステップサイズパラメータと呼ばれる。これを用いて、フィルタの更新式は、次式のようになる。

$$

\bm{w}_k &= \bm{w}_{k-1} + \Delta \bm{w}
         &= \bm{w}_{k-1} + \mu (\bm{r}_{xd} - \bm{R}_x \bm{w}_{k-1})

$$

また、式@は次式のように書き直すことができる。


$$

\bm{w}_k &= \bm{w}_{k-1} + \mu (E[\bm{x}_k d_k^*] - E[\bm{x}_k \bm{x}_^H] \bm{w}_{k-1})
         &= \bm{w}_{k-1} + \mu E[\bm{x}_k (d_k^* - \bm{x}_k^H \bm{w}_{k-1})]

$$

ここで、次式の事前推定誤差を定義する。

$$

e_k := d_k - \bm{w}_{k-1}^H \bm{x}_k

$$

式@二個前 に式@一個前 を代入して、次式を得る。


$$

\bm{w}_k = \bm{w}_{k-1} + \mu E[\bm{x}_k e_k^*]

$$